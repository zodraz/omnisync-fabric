{"cells":[{"cell_type":"code","source":["from datetime import datetime\n","from pyspark.sql.types import MapType,StringType,IntegerType,TimestampType,DoubleType,FloatType,DecimalType,StructType,StructField\n","import notebookutils\n","import traceback\n","\n","spark.conf.set('spark.sql.caseSensitive', True)\n","logger = sc._jvm.org.apache.log4j.LogManager.getLogger(\"com.omnisync.Logger\")\n","\n","def decodeEntity(operation,entity,dict):\n","    if entity == 'Currency':\n","        if operation == 'insert': \n","            df = spark.sql(\"SELECT NVL(MAX(CurrencyKey),0)+1 AS NextCurrencyKey FROM \" + \"OmniSync_DE_LH_300_Gold_Contoso.dbo.\" + entity )\n","            nextValue = df.first()['NextCurrencyKey']\n","            logger.info('Next value is '+ str(nextValue))\n","            staged_rows = [(nextValue,dict['CurrencyName'],dict['CurrencyDescription'], dict['ConversionRate'],\n","                           datetime.now(), None)]\n","        elif operation == 'update':\n","            df = spark.sql(\"SELECT CurrencyKey,CreatedDate FROM OmniSync_DE_LH_300_Gold_Contoso.dbo.\" + entity + \\\n","                           \" WHERE CurrencyName='\" + dict['CurrencyName'] + \"'\")\n","\n","            df.show()\n","            if df.first() == None:\n","                raise Exception(\"Currency \" + dict['CurrencyName'] + \" has not been found.\")\n","\n","            currencyKeyValue = df.first()['CurrencyKey']\n","            createdDateValue = df.first()['CreatedDate']\n","\n","            staged_rows = {\"CurrencyKey\": lit(currencyKeyValue), \"CurrencyName\": lit(dict['CurrencyName']),\n","                           \"CurrencyDescription\": lit(dict['CurrencyDescription']), \"ConversionRate\": lit(dict['ConversionRate']),\n","                           \"CreatedDate\": lit(createdDateValue), \"UpdatedDate\":  lit(datetime.now())}\n","    elif entity == 'Customer':\n","        geometryKey = notebookutils.notebook.run(\"OmniSync_DE_NB_GeographyCDC\", 90, {\"latitude\": dict['Latitude'], \"longitude\": dict['Longitude'] })\n","        #geometryKey = notebookutils.notebook.run(\"OmniSync_DE_NB_GeographyCDC\", 10, {\"latitude\":23, \"longitude\": 44 })\n","        \n","        if str(dict['CreatedDate']).isdigit():\n","            createdDateTimeStamp = datetime.datetime.fromtimestamp(dict['CreatedDate'])\n","            print(createdDateTimeStamp.strftime('%Y-%m-%d %H:%M:%S'))\n","\n","        if str(dict['UpdatedDate']).isdigit():\n","            updatedDateTimeStamp = datetime.datetime.fromtimestamp(dict['UpdatedDate'])\n","            print(updatedDateTimeStamp.strftime('%Y-%m-%d %H:%M:%S'))\n","\n","        if operation == 'insert':\n","            df = spark.sql(\"SELECT NVL(MAX(CustomerKey),0)+1 AS NextKey FROM OmniSync_DE_LH_300_Gold_Contoso.dbo.Customer\")\n","            nextValue = df.first()['NextKey']\n","            logger.info('Next value is '+ str(nextValue))\n","\n","            staged_rows = [(nextValue,geometryKey, None,None,None,None,None,\n","                            dict['EmailAddress'],None,None,None,None,None,None,None,\n","                            dict['AddressLine1'], dict['Phone'],None,'Company', dict['CompanyName'],\n","                            createdDateTimeStamp,updatedDateTimeStamp)]\n","        elif operation == 'update':\n","            df = spark.sql(\"SELECT CustomerKey,CreatedDate FROM OmniSync_DE_LH_300_Gold_Contoso.dbo.Customer\" + \\\n","                           \" WHERE CustomerKey=\" + dict['CustomerKey'])\n","            df.show()\n","            if df.first() == None:\n","                raise Exception(entity + \" with CustomerKey \" + dict['CustomerKey'] + \\\n","                                \" has not been found.\")\n","\n","            keyValue = df.first()['CustomerKey']\n","            createdDateValue = df.first()['CreatedDate']\n","\n","            if str(createdDateValue).isdigit():\n","                createdDateTimeStamp = datetime.datetime.fromtimestamp(createdDateValue)\n","                print(createdDateTimeStamp.strftime('%Y-%m-%d %H:%M:%S'))\n","\n","            staged_rows = {\"CustomerKey\": lit(keyValue), \"AddressLine1\": lit(dict['AddressLine1']),\n","                           \"EmailAddress\": lit(dict['EmailAddress']),\"Phone\": lit(dict['Phone']),\n","                           \"CustomerType\": \"Company\",\n","                           \"CompanyName\": lit(dict['CompanyName']), \"CreatedDate\": lit(createdDateTimeStamp), \n","                           \"UpdatedDate\":  lit(updatedDateTimeStamp)}\n","        else:\n","            staged_rows = None\n","    else:\n","       staged_rows = None\n","    return staged_rows\n","\n","def getSchema(entity):\n","    if entity == 'Currency':\n","        return StructType([ \\\n","                StructField(\"CurrencyKey\",IntegerType(),False), \\\n","                StructField(\"CurrencyName\",StringType(),False), \\\n","                StructField(\"CurrencyDescription\",StringType(),True), \\\n","                StructField(\"ConversionRate\",DoubleType(),True), \\\n","                StructField(\"CreatedDate\", TimestampType(), True), \\\n","                StructField(\"UpdatedDate\", TimestampType(), True)\n","            ])\n","    elif entity == 'Customer':\n","        return StructType([ \\\n","                StructField(\"CustomerKey\",IntegerType(),False), \\\n","                StructField(\"GeographyKey\",IntegerType(),False), \\\n","                StructField(\"FirstName\",StringType(),True), \\\n","                StructField(\"LastName\",StringType(),True), \\\n","                StructField(\"BirthDate\",TimestampType(),True), \\\n","                StructField(\"MaritalStatus\",StringType(),True), \\\n","                StructField(\"Gender\",StringType(),True), \\\n","                StructField(\"EmailAddress\",StringType(),True), \\\n","                StructField(\"YearlyIncome\",IntegerType(),True), \\\n","                StructField(\"TotalChildren\",IntegerType(),True), \\\n","                StructField(\"NumberChildrenAtHome\",IntegerType(),True), \\\n","                StructField(\"Education\",StringType(),True), \\\n","                StructField(\"Occupation\",StringType(),True), \\\n","                StructField(\"HouseOwnerFlag\",StringType(),True), \\\n","                StructField(\"NumberCarsOwned\",IntegerType(),True), \\\n","                StructField(\"AddressLine1\",StringType(),True), \\\n","                StructField(\"Phone\",StringType(),True), \\\n","                StructField(\"DateFirstPurchase\",TimestampType(),True), \\\n","                StructField(\"CustomerType\",StringType(),False), \\\n","                StructField(\"CompanyName\",StringType(),True), \\\n","                StructField(\"CreatedDate\", TimestampType(), False), \\\n","                StructField(\"UpdatedDate\", TimestampType(), True)\n","            ])\n","    elif entity == 'Product':\n","        return StructType([ \\\n","                StructField(\"ProductKey\",IntegerType(),False), \\\n","                StructField(\"ProductName\",StringType(),False), \\\n","                StructField(\"ProductDescription\",StringType(),True), \\\n","                StructField(\"ProductSubcategoryKey\",IntegerType(),False), \\\n","                StructField(\"Manufacturer\",StringType(),True), \\\n","                StructField(\"BrandID\",IntegerType(),True), \\\n","                StructField(\"BrandName\",StringType(),True), \\\n","                StructField(\"ClassID\",IntegerType(),True), \\\n","                StructField(\"ClassName\",StringType(),True), \\\n","                StructField(\"ColorID\",IntegerType(),True), \\\n","                StructField(\"ColorName\",StringType(),True), \\\n","                StructField(\"Size\",StringType(),True), \\\n","                StructField(\"SizeUnitMeasureID\",IntegerType(),True), \\\n","                StructField(\"SizeMeasureName\",StringType(),True), \\\n","                StructField(\"Weight\",FloatType(),True), \\\n","                StructField(\"WeightUnitMeasureID\",IntegerType(),True), \\\n","                StructField(\"WeightUnitMeasureName\",StringType(),True), \\\n","                StructField(\"UnitCost\",DecimalType(),False), \\\n","                StructField(\"UnitPrice\",DecimalType(),False), \\\n","                StructField(\"CreatedDate\", TimestampType(), False), \\\n","                StructField(\"UpdatedDate\", TimestampType(), True)\n","            ])\n","    elif entity == 'SalesOrders':\n","        return StructType([ \\\n","                StructField(\"SalesOrdersKey\",IntegerType(),False), \\\n","                StructField(\"DateKey\",TimestampType(),False), \\\n","                StructField(\"StoreKey\",IntegerType(),False), \\\n","                StructField(\"ProductKey\",IntegerType(),False), \\\n","                StructField(\"CurrencyKey\",IntegerType(),False), \\\n","                StructField(\"CustomerKey\",IntegerType(),False), \\\n","                StructField(\"SalesOrderNumber\",StringType(),False), \\\n","                StructField(\"SalesOrderLineNumber\",StringType(),False), \\\n","                StructField(\"SalesQuantity\",IntegerType(),False), \\\n","                StructField(\"SalesAmount\",DecimalType(),True), \\\n","                StructField(\"TotalCost\",DecimalType(),False), \\\n","                StructField(\"UnitCost\",DecimalType(),False), \\\n","                StructField(\"UnitPrice\",DecimalType(),False), \\\n","                StructField(\"CreatedDate\", TimestampType(), False), \\\n","                StructField(\"UpdatedDate\", TimestampType(), True)\n","            ])\n","    elif entity == 'Sales':\n","        return StructType([ \\\n","                StructField(\"SalesKey\",IntegerType(),False), \\\n","                StructField(\"DateKey\",TimestampType(),False), \\\n","                StructField(\"StoreKey\",IntegerType(),False), \\\n","                StructField(\"ProductKey\",IntegerType(),False), \\\n","                StructField(\"CurrencyKey\",IntegerType(),False), \\\n","                StructField(\"UnitCost\",DecimalType(),False), \\\n","                StructField(\"UnitPrice\",DecimalType(),False), \\\n","                StructField(\"SalesQuantity\",IntegerType(),False), \\\n","                StructField(\"TotalCost\",DecimalType(),False), \\\n","                StructField(\"SalesAmount\",DecimalType(),True), \\\n","                StructField(\"CreatedDate\", TimestampType(), False), \\\n","                StructField(\"UpdatedDate\", TimestampType(), True)\n","            ])\n","    elif entity == 'Store':\n","        return StructType([ \\\n","                StructField(\"StoreKey\",IntegerType(),False), \\\n","                StructField(\"GeographyKey\",IntegerType(),False), \\\n","                StructField(\"StoreTypeID\",IntegerType(),False), \\\n","                StructField(\"StoreType\",StringType(),False), \\\n","                StructField(\"StoreName\",StringType(),True), \\\n","                StructField(\"StoreDescription\",StringType(),False), \\\n","                StructField(\"StorePhone\",StringType(),False), \\\n","                StructField(\"StoreFax\",StringType(),False), \\\n","                StructField(\"AddressLine1\",StringType(),True), \\\n","                StructField(\"AddressLine2\",StringType(),False), \\\n","                StructField(\"EmployeeCount\",IntegerType(),False), \\\n","                StructField(\"Longitude\",DoubleType(),False), \\\n","                StructField(\"Latitude\",DoubleType(),True), \\\n","                StructField(\"CreatedDate\", TimestampType(), True), \\\n","                StructField(\"UpdatedDate\", TimestampType(), True)\n","            ])\n","    elif entity == 'MasterDataMapping':\n","        return StructType([ \\\n","                StructField(\"MasterDataMappingKey\",IntegerType(),False), \\\n","                StructField(\"FabricId\",StringType(),False), \\\n","                StructField(\"SalesForceId\",StringType(),False), \\\n","                StructField(\"SAPId\",StringType(),False), \\\n","                StructField(\"StoreName\",StringType(),True), \\\n","                StructField(\"Entity\",StringType(),False), \\\n","                StructField(\"Name\",StringType(),False), \\\n","                StructField(\"CreatedDate\", TimestampType(), True), \\\n","                StructField(\"UpdatedDate\", TimestampType(), True)\n","            ])\n","        \n","    else:\n","       return None \n","\n","def checkIfInsertNeeded(table, key, value):\n","    df = spark.sql(\"SELECT * FROM OmniSync_DE_LH_300_Gold_Contoso.dbo.\" + table + \" WHERE \" + key + \"='\" + value + \"'\")\n","    return False if df.count() > 0 else True\n","\n","def insertEntity(row, entity):\n","    staged_df = spark.createDataFrame(row,getSchema(entity)) \\\n","                        .write.mode(\"append\").format(\"delta\").saveAsTable(entity)\n","        \n","def getNaturalKey(entity):\n","    if entity == 'Currency':\n","        return 'CurrencyName'\n","    else: \n","        return entity + 'Key'\n","\n","def getPrimaryKey(entity):\n","    if entity == 'Currency':\n","        return 'CurrencyName'\n","    else: \n","        return entity + 'Key'\n","\n","def fixJson(jsonString):\n","    jsonString = jsonString.replace(\"\\t\",\"\")\n","    jsonString = jsonString.replace(\"\\r\\n\",\"\")\n","\n","    print(jsonString)\n","    \n","    return jsonString\n","\n","def mergeMasterDataMapping(dict, entity, key):\n","    salesForceId = dict['SalesForceId'] \n","    SAPId = dict['SAPId']\n","\n","    if salesForceId != None: \n","        df_mapping = spark.sql(\"SELECT * FROM OmniSync_DE_LH_300_Gold_Contoso.dbo.MasterDataMapping \"\\\n","                    \"WHERE Entity='\"+ entity +\"' AND SalesForceId='\" + salesForceId +\"'\" )\n","    elif SAPId != None: \n","        df_mapping = spark.sql(\"SELECT * FROM OmniSync_DE_LH_300_Gold_Contoso.dbo.MasterDataMapping \"\\\n","                    \"WHERE Entity='\"+ entity +\"' AND SAPId='\" + SAPId + \"'\" )\n","    else:\n","        raise Exception(\"No SAPId or SalesForceId on CDC row\")\n","\n","    if df_mapping.count() == 0 :\n","\n","        df_mapping_next = spark.sql(\"SELECT NVL(MAX(MasterDataMappingKey),0)+1 AS NextKey FROM \\\n","                                        OmniSync_DE_LH_300_Gold_Contoso.dbo.MasterDataMapping\")\n","        nextValueDataMapping = df_mapping_next.first()['NextKey']\n","        logger.info('Next value for MasterDataMapping is '+ str(nextValueDataMapping))\n","\n","        if salesForceId != None: \n","            mapping_rows = [(nextValueDataMapping, nextValue, salesForceId , None, \\\n","                            '\"+ entity +\"', key , createdDateTimeStamp,updatedDateTimeStamp)]\n","        elif SAPId != None: \n","            mapping_rows = [(nextValueDataMapping, nextValue, None, SAPId , \\\n","                            '\"+ entity +\"', key , createdDateTimeStamp,updatedDateTimeStamp)]\n","\n","        spark.createDataFrame(mapping_rows, getSchema('MasterDataMapping')) \\\n","             .write.mode(\"append\").format(\"delta\").saveAsTable('MasterDataMapping')\n","        logger.info('Created MasterDataMapping: ' + mapping_rows)\n","\n","    else:\n","\n","        logger.info('Mapping with SalesForceId:' + dict['SalesForceId'] + ' already in the system. Updating...')\n","\n","        id = df_mapping['MasterDataMappingKey']\n","\n","        if (salesForceId == None ):\n","            salesForceId =  df_mapping['SalesForceId'] \n","\n","        if (SAPId == None ):\n","            SAPId =  df_mapping['SAPId'] \n","\n","        updatedDateValue = dict['UpdatedDate']\n","\n","        if str(updatedDateValue).isdigit():\n","            updatedDateTimeStamp = datetime.datetime.fromtimestamp(updatedDateValue)\n","            print(updatedDateTimeStamp.strftime('%Y-%m-%d %H:%M:%S'))\n","    \n","        mapping_rows = [(id, df_mapping['FabricId'], salesForceId , SAPId, \\\n","                            'MasterDataMapping', df_mapping['Name'] , \\\n","                            df_mapping['CreatedDate'],updatedDateTimeStamp)]\n","\n","        logger.info('Update key to check: ' + dict[pk])\n","        deltaTable = DeltaTable.forPath(spark, 'Tables/dbo/MasterDataMapping')\n","\n","        deltaTable.update(\n","            condition = col('MasterDataMappingKey') == id,\n","            set = mapping_rows\n","        )\n","\n","def getFabricIdFromSalesForceOrSAPId(dict):\n","\n","    salesForceId = dict['SalesForceId']\n","    df = spark.sql(\"SELECT * FROM OmniSync_DE_LH_300_Gold_Contoso.dbo.MasterDataMapping LIMIT 1000\")\n","    display(df)\n","\n","\n","def handleException(logger,  ex: Exception):\n","    # By this way we can know about the type of error occurring\n","    ex_t = type(ex).__name__\n","    err = str(ex)\n","    err_msg = f'[{ex_t}] - {err}'\n","    print(err_msg)\n","    logger.error(err_msg)\n","    # go through the trackback lines and individually add those to the log as an error\n","    for l in traceback.format_exc().splitlines():\n","        logger.error(l)\n","        print(l)\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"0db808bc-4ea8-4d3c-9810-7a6f9f572168","normalized_state":"finished","queued_time":"2025-03-17T18:46:08.3904284Z","session_start_time":null,"execution_start_time":"2025-03-17T18:46:08.3929274Z","execution_finish_time":"2025-03-17T18:46:09.1469189Z","parent_msg_id":"179273e0-a292-4afe-ae35-9c6e57eb6d3b"},"text/plain":"StatementMeta(, 0db808bc-4ea8-4d3c-9810-7a6f9f572168, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b6cf11cc-c02a-4063-bd1f-3ee4d9b7ff20"},{"cell_type":"code","source":["from pyspark.sql.types import * \n","from delta.tables import *\n","from pyspark.sql.functions import *\n","import pandas as pd\n","from pyspark.sql.functions import from_json\n","import json\n","import traceback\n","\n","spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n","logger = sc._jvm.org.apache.log4j.LogManager.getLogger(\"com.omnisync.Logger\")\n","\n","externalCDCSchema = spark.read.parquet(\"Tables/dbo/ExternalCDC\").schema\n","\n","df = spark.readStream.schema(externalCDCSchema).format(\"parquet\").option(\"path\", \"Tables/dbo/ExternalCDC\").load()\n","\n","def sendToSinkTable(df, epoch_id):\n","    \n","    print('------------------------------Stream received---------------------------------------')\n","    \n","    try:\n","        dataCollect = df.collect()\n","        for row in dataCollect:\n","            logger.info(row)\n","            try:\n","                operation=row.Operation.lower()\n","                entity=row.Entity\n","                values = row.Values\n","                print(\"-----------------------Step 0-------------------------\")\n","                dict = json.loads(fixJson(values))\n","                print(\"-----------------------Step 1-------------------------\")\n","                naturalKey = getNaturalKey(entity)\n","                pk = getPrimaryKey(entity)  \n","\n","                if operation == 'create':         \n","                    logger.info('Create key to check: ' + dict[pk])\n","                    entityRow = decodeEntity('Insert', entity, dict)\n","                    if (entityRow == None):\n","                        logger.info(\"Could not decode entity: \" + entity )\n","                        continue\n","\n","                    if checkIfInsertNeeded(entity, pk, dict[pk]):\n","                        insertEntity(entityRow, entity)\n","                        logger.info('Created ' + entity + \": \" + entityRow)\n","                    else:\n","                        logger.info(entity + ' with id:' + dict[pk] + \\\n","                                    ' already in the system. Skipping insert...')\n","                        \n","                    mergeMasterDataMapping(dict, entity, naturalKey)\n","                elif operation == 'update':\n","                    logger.info('Update key to check: ' + dict[pk])\n","                    deltaTable = DeltaTable.forPath(spark, 'Tables/dbo/'+ entity)\n","                    entityRow = decodeEntity('Update', entity, dict)\n","                    if (entityRow == None):\n","                        logger.info(\"Could not decode entity: \" + entity )\n","                        continue\n","\n","                    deltaTable.update(\n","                        condition = col(pk) == dict[pk],\n","                        set = entityRow\n","                    )\n","                    mergeMasterDataMapping(dict, entity, naturalKey)\n","                elif operation == 'delete':\n","                    logger.info('Delete key to check: ' + dict[pk])\n","                    deltaTable = DeltaTable.forPath(spark, 'Tables/dbo/'+ entity)\n","                    deltaTable.delete(col(pk) == dict[pk])\n","                    mergeMasterDataMapping(dict, entity, naturalKey)\n","                else:\n","                    logger.info('Error. Opration not recognized: ' + operation)\n","            except Exception as ex:\n","                print(\"-----------------------Error-------------------------\")\n","                handleException(logger, ex)\n","    except Exception as e:\n","        # By this way we can know about the type of error occurring\n","        print(\"-----------------------Outside Error-------------------------\")\n","        logger.error(\"The error is: outer-------\")\n","\n","logger.info('------------------------------Starting---------------------------------------')\n","\n","df.writeStream \\\n","    .outputMode(\"append\") \\\n","    .trigger(processingTime='10 seconds') \\\n","    .option(\"checkpointLocation\",\"Files/__checkpoint__\") \\\n","    .format(\"delta\") \\\n","    .foreachBatch(sendToSinkTable) \\\n","    .start() \\\n","    .awaitTermination()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"submitted","livy_statement_state":"running","session_id":"0db808bc-4ea8-4d3c-9810-7a6f9f572168","normalized_state":"running","queued_time":"2025-03-17T18:52:09.5152706Z","session_start_time":null,"execution_start_time":"2025-03-17T18:52:09.5171845Z","execution_finish_time":null,"parent_msg_id":"2d173028-5846-4018-8c50-44135dfbe1a7"},"text/plain":"StatementMeta(, 0db808bc-4ea8-4d3c-9810-7a6f9f572168, 20, Submitted, Running, Running)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["------------------------------Stream received---------------------------------------\n-----------------------Step 0-------------------------\n{            \"SalesForceId\": \"001d1000009ocrPAAQ\",\"CompanyName\": \"kk\",\"EmailAddress\": \"\",\"Phone\": \"\",\"CustomerType\": \"Company\",\"AddressLine1\": \"    \", \"Latitude\": \"\",\"Longitude\": \"\"  }\n-----------------------Step 1-------------------------\n-----------------------Error-------------------------\n[KeyError] - 'CustomerKey'\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_9586/3228181397.py\", line 35, in sendToSinkTable\n    logger.info('Create key to check: ' + dict[pk])\n                                          ~~~~^^^^\nKeyError: 'CustomerKey'\n"]}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d8aae9a7-5feb-4321-bd80-3b18a7598bae"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"e855e388-3034-499e-a5ee-1808e036efbd","default_lakehouse_name":"OmniSync_DE_LH_300_Gold_Contoso","default_lakehouse_workspace_id":"6b35ae7a-875a-4e1a-8f60-9f0a22d0e0d0"},"environment":{"environmentId":"73f3f167-975b-4745-aef9-dbbd4d7e0d58","workspaceId":"6b35ae7a-875a-4e1a-8f60-9f0a22d0e0d0"}}},"nbformat":4,"nbformat_minor":5}